base_model: Llama-3.2B-Math-Mamba-Untrained
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
tokenizer_config: Llama-3.2B-Math-Mamba-Untrained
mamba: true

strict: false

datasets:
- name: tokenized_llama31
  type:
  path: JunxiongWang/MATH_SFT
  split: train
  train_on_split: train

dataset_prepared_path: MATH-SFT-tokenized
output_dir: M1-3B-MATH-Distill

sequence_len: 8192
sample_packing: true
eval_sample_packing: false
pad_to_sequence_len: true
auto_resume_from_checkpoints: true
evaluation_strategy: "no"

wandb_mode:
wandb_project: M1
# wandb_entity: YOUR_WANDB
wandb_watch:
wandb_name: M1-Math-Distill
wandb_log_model:

ssm_layers: [0, 1, 2, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26]

gradient_accumulation_steps: 1
micro_batch_size: 1
eval_batch_size: 1
num_epochs: 2
optimizer: adamw_torch
lr_scheduler: cosine
learning_rate: 1e-5
warmup_steps: 200
max_grad_norm: 1.0

distillation: true
teacher_model: "meta-llama/Llama-3.2-3B-Instruct"
distillation_loss_type: "reverse_kl"
kl_weight: 1.0

train_on_inputs: false
group_by_length: false
bf16: true
fp16:
tf32: true

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: true
early_stopping_patience:
logging_steps: 1
xformers_attention:
flash_attention: true

save_total_limit: 3
save_strategy: steps
save_steps: 10000
save_safetensors: true
debug:
deepspeed: deepspeed_configs/zero2_nooffload.json
weight_decay: 0.1

special_tokens:
  pad_token: <|finetune_right_pad_id|>
  eos_token: <|eot_id|>
  bot_token: <|begin_of_text|>